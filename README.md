# ğŸ¤– RAG Application - AI Document Assistant

A powerful Retrieval-Augmented Generation (RAG) application built with FastAPI, ChromaDB, and Ollama for intelligent document processing and AI-powered question answering.

## ğŸš€ Features

### **Core Functionality**
- **ğŸ“„ Document Upload**: Support for PDF, TXT, MD, and JSON files
- **ğŸ” Semantic Search**: Advanced vector-based document search
- **ğŸ§  AI-Powered Q&A**: Intelligent question answering using Ollama
- **ğŸ“Š Real-time Streaming**: Live AI response streaming
- **ğŸ¯ Context-Aware**: Uses relevant document context for accurate answers

### **Advanced Features**
- **ğŸ“ˆ Comprehensive Monitoring**: Real-time system and Ollama monitoring
- **ğŸ”’ Security**: Rate limiting and security headers
- **ğŸ“± Modern UI**: Responsive web interface
- **âš¡ Performance Optimized**: Efficient Ollama integration
- **ğŸ”„ Auto-refresh**: Real-time dashboard updates

### **Monitoring & Analytics**
- **ğŸ“Š System Metrics**: CPU, memory, disk usage monitoring
- **ğŸ¤– Ollama Monitoring**: Model usage, response times, performance tracking
- **ğŸ“ Structured Logging**: JSON-formatted logs with rotation
- **ğŸ¯ Health Checks**: Comprehensive application health monitoring
- **ğŸ“ˆ Performance Analytics**: Request/response time tracking

## ğŸ› ï¸ Technology Stack

- **Backend**: FastAPI (Python)
- **Vector Database**: ChromaDB
- **AI Models**: Ollama (Local LLM)
- **Embeddings**: Sentence Transformers
- **Frontend**: HTML/CSS/JavaScript
- **Monitoring**: Custom monitoring system with real-time dashboards

## ğŸ“‹ Prerequisites

- **Python 3.8+**
- **Ollama** installed and running
- **Git** for cloning the repository

### **Ollama Setup**
```bash
# Install Ollama (Linux/macOS)
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service
ollama serve

# Pull a model (in another terminal)
ollama pull gemma3:1b
ollama pull nomic-embed-text:v1.5
```

## ğŸš€ Quick Start

### **1. Clone the Repository**
```bash
git clone <repository-url>
cd fastapi_rag_app
```

### **2. Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3. Start the Application**
```bash
python -m uvicorn main:app --reload --port 8000
```

### **4. Access the Application**
- **Main Application**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs
- **System Monitoring**: http://localhost:8000/monitoring-dashboard
- **Ollama Monitoring**: http://localhost:8000/ollama-dashboard

## ğŸ“– Usage Guide

### **1. Upload Documents**
1. Navigate to the main application
2. Drag and drop files or click "Choose Files"
3. Supported formats: PDF, TXT, MD, JSON
4. Documents are automatically processed and indexed

### **2. Search Documents**
1. Use the search section in the sidebar
2. Enter your search query
3. Adjust similarity threshold and result count
4. View relevant document snippets

### **3. Ask AI Questions**
1. Type your question in the main AI assistant section
2. The AI will use uploaded documents as context
3. Receive comprehensive, detailed answers
4. Watch real-time streaming responses

### **4. Monitor Performance**
1. Access monitoring dashboards from the sidebar
2. View real-time system metrics
3. Track Ollama performance and usage
4. Monitor application health

## ğŸ”§ Configuration

### **Environment Variables**
Create a `.env` file for custom configuration:
```env
OLLAMA_HOST=http://localhost:11434
CHROMADB_PATH=./chromadb_storage
EMBEDDING_MODEL=all-MiniLM-L6-v2
LLM_MODEL=gemma3:1b
MAX_TOKENS=2000
TEMPERATURE=0.7
```

### **Model Configuration**
Edit `config.py` to customize AI behavior:
```python
class Config:
    CHROMADB_PATH = "./chromadb_storage"
    EMBEDDING_MODEL = "all-MiniLM-L6-v2"
    LLM_MODEL = "gemma3:1b"
    MAX_TOKENS = 2000
    TEMPERATURE = 0.7
    MIN_RESPONSE_TOKENS = 50
    MAX_RESPONSE_TOKENS = 1500
```

## ğŸ“Š Monitoring & Analytics

### **System Monitoring Dashboard**
- **Health Status**: Application and service health
- **Performance Metrics**: Response times and success rates
- **System Resources**: CPU, memory, disk usage
- **Request Analytics**: Endpoint usage statistics

### **Ollama Monitoring Dashboard**
- **Model Status**: Active models and availability
- **Performance Tracking**: Response times and throughput
- **Resource Usage**: CPU and memory consumption
- **Request History**: Usage patterns and trends

### **API Endpoints**
```bash
# Health checks
GET /monitoring/health
GET /monitoring/health/detailed

# Metrics
GET /monitoring/metrics
GET /monitoring/metrics/performance
GET /monitoring/metrics/system

# Ollama monitoring
GET /monitoring/ollama/status
GET /monitoring/ollama/realtime

# Logs
GET /monitoring/logs/recent
GET /monitoring/logs/errors
```

## ğŸ”’ Security Features

- **Rate Limiting**: 200 requests per minute per IP
- **Security Headers**: XSS protection, content type options
- **Input Validation**: Comprehensive request validation
- **Error Handling**: Secure error responses

## ğŸ“ Project Structure

```
fastapi_rag_app/
â”œâ”€â”€ main.py                 # FastAPI application entry point
â”œâ”€â”€ config.py              # Configuration settings
â”œâ”€â”€ llm.py                 # Ollama integration and AI logic
â”œâ”€â”€ rag.py                 # RAG pipeline implementation
â”œâ”€â”€ database.py            # ChromaDB database operations
â”œâ”€â”€ pdf_scraper.py         # PDF processing utilities
â”œâ”€â”€ models.py              # Pydantic data models
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ frontend.html          # Main web interface
â”œâ”€â”€ monitoring_dashboard.html  # System monitoring dashboard
â”œâ”€â”€ ollama_dashboard.html  # Ollama monitoring dashboard
â”œâ”€â”€ routes/                # API route modules
â”‚   â”œâ”€â”€ documents.py       # Document management routes
â”‚   â”œâ”€â”€ search.py          # Search functionality routes
â”‚   â”œâ”€â”€ rag_routes.py      # RAG processing routes
â”‚   â”œâ”€â”€ upload.py          # File upload routes
â”‚   â”œâ”€â”€ health.py          # Health check routes
â”‚   â””â”€â”€ monitoring.py      # Monitoring API routes
â”œâ”€â”€ utils/                 # Utility modules
â”‚   â”œâ”€â”€ logger.py          # Logging configuration
â”‚   â”œâ”€â”€ monitoring.py      # Monitoring system
â”‚   â”œâ”€â”€ middleware.py      # HTTP middleware
â”‚   â””â”€â”€ ollama_monitor.py  # Ollama monitoring
â””â”€â”€ chromadb_storage/      # Vector database storage
```

## ğŸš€ Performance Optimization

### **Recent Improvements**
- **Eliminated infinite loops** in AI generation
- **Removed artificial delays** in streaming responses
- **Optimized token limits** for faster responses
- **Smart model management** with availability checking
- **Efficient retry logic** with controlled attempts

### **Performance Tips**
1. **Use appropriate models** for your use case
2. **Monitor resource usage** via dashboards
3. **Optimize document size** for faster processing
4. **Adjust token limits** based on response quality needs

## ğŸ› Troubleshooting

### **Common Issues**

#### **Ollama Not Responding**
```bash
# Check if Ollama is running
ollama list

# Restart Ollama service
ollama serve
```

#### **Model Not Found**
```bash
# Pull the required model
ollama pull gemma3:1b

# Check available models
ollama list
```

#### **High Memory Usage**
- Monitor via Ollama dashboard
- Consider using smaller models
- Adjust token limits in config.py

#### **Slow Response Times**
- Check CPU usage in monitoring dashboard
- Verify Ollama service status
- Consider model optimization

### **Logs and Debugging**
- **Application logs**: `logs/app.log`
- **Monitoring data**: Available via API endpoints
- **Error tracking**: Built-in error monitoring

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Ollama** for local LLM capabilities
- **ChromaDB** for vector database functionality
- **FastAPI** for the web framework
- **Sentence Transformers** for embeddings

## ğŸ“ Support

For issues and questions:
1. Check the troubleshooting section
2. Review the monitoring dashboards
3. Check application logs
4. Open an issue on GitHub

---

**Made with â¤ï¸ for intelligent document processing and AI-powered insights**
